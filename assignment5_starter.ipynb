{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the KNN class\n",
    "\n",
    "class KNN:\n",
    "    def __init__(self, k=3, distance_metric='euclidean'):\n",
    "        self.k = k\n",
    "        self.distance_metric = distance_metric\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # Store the training data\n",
    "        self.X = np.array(X)\n",
    "        self.y = np.array(y)\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Make predictions based on the nearest neighbors.\"\"\"\n",
    "        X = np.array(X)\n",
    "    \n",
    "        # Ensure X is 2D (in case a single sample is passed)\n",
    "        if X.ndim == 1:\n",
    "            X = X.reshape(1, -1)\n",
    "        \n",
    "        # Ensure self.X (training data) is 2D\n",
    "        if self.X.ndim == 1:\n",
    "            self.X = self.X.reshape(1, -1)\n",
    "    \n",
    "        # Compute distances between test set and training set\n",
    "        distances = self.compute_distance(X, self.X)\n",
    "        \n",
    "        probabilities = []\n",
    "        for i in range(X.shape[0]):\n",
    "            # Get indices of k nearest neighbors\n",
    "            nearest_neighbors_indices = np.argsort(distances[i])[:self.k]\n",
    "            nearest_labels = self.y[nearest_neighbors_indices]\n",
    "            \n",
    "            # Calculate the probability as the fraction of neighbors that are class 1 (churn)\n",
    "            probability = np.mean(nearest_labels)  # The proportion of neighbors that are '1'\n",
    "            probabilities.append(probability)\n",
    "        \n",
    "        return np.array(probabilities)  # Return probabiliti\n",
    "\n",
    "        \n",
    "    def compute_distance(self, X1, X2):\n",
    "        \"\"\"\n",
    "        Compute distance matrix based on the selected distance metric.\n",
    "        This function computes pairwise distances between each row in X1 and each row in X2.\n",
    "        \"\"\"\n",
    "        # Ensure inputs are numpy arrays and convert them to float\n",
    "        X1 = np.array(X1).astype(float)\n",
    "        X2 = np.array(X2).astype(float)\n",
    "    \n",
    "    \n",
    "        # Step 1: Compute squared sum of each row in X1 and X2\n",
    "        X1_squared = np.sum(np.square(X1), axis=1).reshape(-1, 1)  # Shape: (m_test, 1)\n",
    "        X2_squared = np.sum(np.square(X2), axis=1).reshape(1, -1)  # Shape: (1, m_train)\n",
    "    \n",
    "        # Step 2: Compute the dot product between X1 and X2\n",
    "        cross_term = np.dot(X1, X2.T)  # Shape: (m_test, m_train)\n",
    "    \n",
    "        # Step 3: Compute the full distance matrix using broadcasting\n",
    "        distance_matrix = X1_squared + X2_squared - 2 * cross_term\n",
    "    \n",
    "        # Safety check to ensure no negative values under the square root\n",
    "        distance_matrix = np.maximum(distance_matrix, 0)\n",
    "    \n",
    "        # Ensure the input to np.isnan is numeric and safe to check\n",
    "        if np.isnan(distance_matrix).any():\n",
    "            raise ValueError(\"NaN values found in distance matrix\")\n",
    "    \n",
    "        # Element-wise square root to get the final Euclidean distances\n",
    "        distances = np.sqrt(distance_matrix)\n",
    "    \n",
    "    \n",
    "        return distances\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tune the decision threshold for predicting churn\n",
    "def tune_threshold(y_true, y_pred_probs):\n",
    "    thresholds = np.linspace(0, 1, 101)\n",
    "    best_threshold = 0.5\n",
    "    best_score = 0\n",
    "    \n",
    "    for threshold in thresholds:\n",
    "        y_pred = (y_pred_probs >= threshold).astype(int)\n",
    "        score = np.mean(y_true == y_pred)  # Accuracy score\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_threshold = threshold\n",
    "    \n",
    "    return best_threshold\n",
    "\n",
    "\n",
    "def handle_class_imbalance(X, y):\n",
    "    data = pd.DataFrame(X)\n",
    "    data['target'] = y\n",
    "    majority_class = data[data['target'] == 0]\n",
    "    minority_class = data[data['target'] == 1]\n",
    "    \n",
    "    # Oversample the minority class by replicating rows\n",
    "    minority_upsampled = minority_class.sample(n=len(majority_class), replace=True, random_state=42)\n",
    "    upsampled_data = pd.concat([majority_class, minority_upsampled])\n",
    "    upsampled_data = upsampled_data.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "    \n",
    "    # Separate X and y again\n",
    "    X_resampled = upsampled_data.drop(columns=['target']).to_numpy()\n",
    "    y_resampled = upsampled_data['target'].to_numpy()\n",
    "    \n",
    "    return X_resampled, y_resampled\n",
    "\n",
    "# Add feature engineering (e.g., age groups, balance bins)\n",
    "def feature_engineering(df):\n",
    "    bins = [18, 30, 45, 60, 100]\n",
    "    labels = ['18-30', '31-45', '46-60', '60+']\n",
    "    df['AgeGroup'] = pd.cut(df['Age'], bins=bins, labels=labels, right=False)\n",
    "    df['BalanceBin'] = np.where(df['Balance'] == 0, 'Zero', 'Non-zero')\n",
    "    return df\n",
    "\n",
    "def impute_mean(column):\n",
    "    \"\"\"Replace missing numerical values with the column mean.\"\"\"\n",
    "    return column.fillna(column.mean())\n",
    "\n",
    "def min_max_scale(df):\n",
    "    \"\"\"Apply Min-Max scaling to the DataFrame.\"\"\"\n",
    "    return (df - df.min()) / (df.max() - df.min())\n",
    "\n",
    "def impute_most_frequent(column):\n",
    "    \"\"\"Replace missing categorical values with the most frequent value.\"\"\"\n",
    "    return column.fillna(column.mode()[0])\n",
    "\n",
    "def one_hot_encode(df, categorical_features):\n",
    "    \"\"\"Perform one-hot encoding for categorical features.\"\"\"\n",
    "    for feature in categorical_features:\n",
    "        one_hot = pd.get_dummies(df[feature], prefix=feature)\n",
    "        df = pd.concat([df.drop(columns=[feature]), one_hot], axis=1)\n",
    "    return df\n",
    "\n",
    "def standard_scale(X, mean=None, std=None):\n",
    "    \"\"\"Standardize features by removing the mean and scaling to unit variance.\"\"\"\n",
    "    if mean is None and std is None:\n",
    "        mean = np.mean(X, axis=0)\n",
    "        std = np.std(X, axis=0)\n",
    "    \n",
    "    # Avoid division by zero for features with zero variance\n",
    "    std[std == 0] = 1\n",
    "    \n",
    "    return (X - mean) / std\n",
    "\n",
    "\n",
    "def pca(X, n_components):\n",
    "    # Step 1: Center the data\n",
    "    X_centered = X - np.mean(X, axis=0)\n",
    "    covariance_matrix = np.cov(X_centered, rowvar=False)\n",
    "    eigenvalues, eigenvectors = np.linalg.eigh(covariance_matrix)\n",
    "    sorted_indices = np.argsort(eigenvalues)[::-1]\n",
    "    top_eigenvectors = eigenvectors[:, sorted_indices[:n_components]]\n",
    "    X_reduced = np.dot(X_centered, top_eigenvectors)\n",
    "    \n",
    "    # Return reduced data and the components (eigenvectors)\n",
    "    return X_reduced, top_eigenvectors, np.mean(X, axis=0)\n",
    "\n",
    "\n",
    "\n",
    "def preprocess_data(train_path, test_path, scaling_type='standard', apply_pca=False, n_components=None):\n",
    "    train_data = pd.read_csv(train_path)\n",
    "    test_data = pd.read_csv(test_path)\n",
    "\n",
    "    # Handle NaN values in a pandas-friendly way (pandas' fillna)\n",
    "    train_data.fillna(0, inplace=True)  # Replace NaNs with 0\n",
    "    test_data.fillna(0, inplace=True)   # Replace NaNs with 0\n",
    "\n",
    "    # Feature engineering (age groups, balance bins)\n",
    "    train_data = feature_engineering(train_data)\n",
    "    test_data = feature_engineering(test_data)\n",
    "\n",
    "    # Drop unnecessary columns\n",
    "    X_train = train_data.drop(columns=['Surname', 'id', 'CustomerId', 'Exited'])\n",
    "    y_train = train_data['Exited']\n",
    "    X_test = test_data.drop(columns=['Surname', 'id', 'CustomerId'])\n",
    "\n",
    "    # Define numerical, categorical, and binary columns\n",
    "    numerical_features = ['CreditScore', 'Age', 'Tenure', 'Balance', 'NumOfProducts', 'EstimatedSalary']\n",
    "    categorical_features = ['Geography', 'AgeGroup', 'BalanceBin']\n",
    "    binary_features = ['HasCrCard', 'IsActiveMember', 'Gender']\n",
    "\n",
    "    # Convert binary features to binary (0 or 1) encoding for Gender\n",
    "    X_train['Gender'] = X_train['Gender'].map({'Male': 0, 'Female': 1})\n",
    "    X_test['Gender'] = X_test['Gender'].map({'Male': 0, 'Female': 1})\n",
    "\n",
    "    # Preprocess numerical features (Impute missing values)\n",
    "    X_train[numerical_features] = X_train[numerical_features].apply(impute_mean, axis=0)\n",
    "    X_test[numerical_features] = X_test[numerical_features].apply(impute_mean, axis=0)\n",
    "\n",
    "    # Apply scaling\n",
    "    if scaling_type == 'standard':\n",
    "        X_train[numerical_features] = standard_scale(X_train[numerical_features])\n",
    "        X_test[numerical_features] = standard_scale(X_test[numerical_features])\n",
    "    elif scaling_type == 'minmax':\n",
    "        X_train[numerical_features] = min_max_scale(X_train[numerical_features])\n",
    "        X_test[numerical_features] = min_max_scale(X_test[numerical_features])\n",
    "\n",
    "    # Apply one-hot encoding to categorical features\n",
    "    X_train = one_hot_encode(X_train, categorical_features)\n",
    "    X_test = one_hot_encode(X_test, categorical_features)\n",
    "\n",
    "    # Convert bool columns to int after one-hot encoding\n",
    "    X_train = X_train.astype({col: 'int' for col in X_train.select_dtypes(include=['bool']).columns})\n",
    "    X_test = X_test.astype({col: 'int' for col in X_test.select_dtypes(include=['bool']).columns})\n",
    "\n",
    "    # Align train and test columns\n",
    "    X_train, X_test = X_train.align(X_test, join='left', axis=1, fill_value=0)\n",
    "\n",
    "    # Apply PCA if specified\n",
    "    if apply_pca and n_components is not None:\n",
    "        X_train, pca_components, train_mean = pca(X_train.to_numpy(), n_components=n_components)\n",
    "        X_test = np.dot(X_test - train_mean, pca_components)\n",
    "\n",
    "    return X_train, X_test, y_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_roc_auc(y_true, y_pred_probs):\n",
    "    \"\"\"\n",
    "    Compute ROC and AUC score from scratch.\n",
    "    \n",
    "    Parameters:\n",
    "    - y_true: Actual binary labels (0 or 1)\n",
    "    - y_pred_probs: Predicted probabilities for the positive class (1)\n",
    "    \n",
    "    Returns:\n",
    "    - auc: Area under the ROC curve\n",
    "    \"\"\"\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred_probs = np.array(y_pred_probs)\n",
    "    \n",
    "    # Step 1: Sort predictions and corresponding true labels by predicted probabilities\n",
    "    sorted_indices = np.argsort(y_pred_probs)[::-1]  # Sort in descending order\n",
    "    y_true_sorted = y_true[sorted_indices]\n",
    "    \n",
    "    # Step 2: Initialize variables to compute TPR and FPR\n",
    "    tpr = []  # True positive rate\n",
    "    fpr = []  # False positive rate\n",
    "    \n",
    "    # Count positives and negatives\n",
    "    P = np.sum(y_true == 1)  # Number of positive examples\n",
    "    N = np.sum(y_true == 0)  # Number of negative examples\n",
    "    \n",
    "    # Initialize true positives and false positives counts\n",
    "    tp = 0  # True positives\n",
    "    fp = 0  # False positives\n",
    "    \n",
    "    # Step 3: Compute TPR and FPR at different threshold levels\n",
    "    for i in range(len(y_true_sorted)):\n",
    "        if y_true_sorted[i] == 1:\n",
    "            tp += 1\n",
    "        else:\n",
    "            fp += 1\n",
    "        \n",
    "        tpr.append(tp / P)  # True positive rate\n",
    "        fpr.append(fp / N)  # False positive rate\n",
    "    \n",
    "    # Step 4: Calculate the AUC using the trapezoidal rule\n",
    "    auc = np.trapz(tpr, fpr)  # Trapezoidal approximation for AUC\n",
    "    return auc\n",
    "\n",
    "\n",
    "def cross_validate(X, y, knn, n_splits=5):\n",
    "    # Convert y to a numpy array and reshape it for concatenation\n",
    "    y = np.array(y).reshape(-1, 1)\n",
    "\n",
    "    # Combine X and y so they can be shuffled together\n",
    "    data = np.hstack((X, y))\n",
    "\n",
    "    # Shuffle the data randomly\n",
    "    np.random.shuffle(data)\n",
    "\n",
    "    # Split X and y back\n",
    "    X = data[:, :-1]\n",
    "    y = data[:, -1]\n",
    "\n",
    "    # Determine fold size\n",
    "    fold_size = len(X) // n_splits\n",
    "\n",
    "    # List to store the AUC scores for each fold\n",
    "    auc_scores = []\n",
    "    for i in range(n_splits):\n",
    "        # Create validation set\n",
    "        X_val = X[i * fold_size:(i + 1) * fold_size]\n",
    "        y_val = y[i * fold_size:(i + 1) * fold_size]\n",
    "\n",
    "\n",
    "        # Create training set (everything except the validation set)\n",
    "        X_train = np.concatenate((X[:i * fold_size], X[(i + 1) * fold_size:]), axis=0)\n",
    "        y_train = np.concatenate((y[:i * fold_size], y[(i + 1) * fold_size:]), axis=0)\n",
    "\n",
    "\n",
    "        # Fit the KNN model on the training data\n",
    "        knn.fit(X_train, y_train)\n",
    "\n",
    "        # Get predicted probabilities for the validation set\n",
    "        y_pred_probs = knn.predict(X_val)\n",
    "\n",
    "        # Compute the ROC AUC score for the validation set\n",
    "        auc_score = calculate_roc_auc(y_val, y_pred_probs)\n",
    "        auc_scores.append(auc_score)\n",
    "\n",
    "    # Return the average AUC score over all splits\n",
    "    return np.mean(auc_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation AUC score: 0.909906692368782 for n_comp: 18\n",
      "Cross-validation AUC score: 0.9126118608904441 for n_comp: 20\n",
      "Cross-validation AUC score: 0.9142143490920855 for n_comp: 22\n",
      "Cross-validation AUC score: 0.9133032057333843 for n_comp: 24\n",
      "Cross-validation AUC score: 0.914146932290615 for n_comp: 26\n",
      "Cross-validation AUC score: 0.9133039229332454 for n_comp: 28\n",
      "Best score: 0.9142143490920855, Best k: 22\n",
      "Test predictions saved to 'submissions.csv'\n"
     ]
    }
   ],
   "source": [
    "# Load and preprocess data\n",
    "\n",
    "\n",
    "best_k = 7\n",
    "best_score = 0\n",
    "\n",
    "# Loop to test different values of n_components (for PCA) and k (for KNN)\n",
    "#for i in range(7, 11):\n",
    " #   X_train, X_test, y_train = preprocess_data('train.csv', 'test.csv', scaling_type='standard', apply_pca=True, n_components=i)\n",
    "X_train, X_test, y_train = preprocess_data('train.csv', 'test.csv', scaling_type='standard',apply_pca = True, n_components = 21)\n",
    "X_train_resampled, y_train_resampled = handle_class_imbalance(X_train, y_train)\n",
    "for k in range(18, 29, 2):  # Try different values of k\n",
    "    knn = KNN(k=k, distance_metric='euclidean')\n",
    "    cv_score = cross_validate(X_train, y_train, knn)\n",
    "    print(f\"Cross-validation AUC score: {cv_score} for n_comp: {k}\")\n",
    "    if cv_score > best_score:\n",
    "        best_score = cv_score\n",
    "        best_k = k\n",
    "# Handle class imbalance\n",
    "\n",
    "print(f\"Best score: {best_score}, Best k: {best_k}\")\n",
    "\n",
    "# Train the KNN model on the resampled training data with the best k (after handling class imbalance)\n",
    "knn = KNN(k=best_k, distance_metric='euclidean')\n",
    "knn.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Predict probabilities for the test set (using the trained model)\n",
    "y_test_pred_probs = knn.predict(X_test)  # Get probabilities for the test set\n",
    "\n",
    "# Save the test predictions as probabilities\n",
    "test_ids = pd.read_csv('test.csv')['id']  # Extract the 'id' column from the test.csv file\n",
    "\n",
    "# Create the submission DataFrame with 'id' and predicted 'Exited' probabilities\n",
    "#submission = pd.DataFrame({'id': test_ids, 'Exited': y_test_pred_probs})\n",
    "pd.DataFrame({'id': pd.read_csv('test.csv')['id'], 'Exited': y_test_pred_probs}).to_csv('submissions.csv', index=False) \n",
    "\n",
    "print(\"Test predictions saved to 'submissions.csv'\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
